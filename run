#!/bin/bash
# SPDX-FileCopyrightText: Copyright (c) 2022-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Error out if a command fails
set -e

#===============================================================================
# Default values for environment variables.
#===============================================================================

init_globals() {
    if [ "$0" != "/bin/bash" ] && [ "$0" != "bash" ]; then
        SCRIPT_DIR=$(dirname "$(readlink -f "$0")")
        export RUN_SCRIPT_FILE="$(readlink -f "$0")"
    else
        export RUN_SCRIPT_FILE="$(readlink -f "${BASH_SOURCE[0]}")"
    fi

    export TOP=$(dirname "${RUN_SCRIPT_FILE}")

    HOLOSCAN_PY_EXE=${HOLOSCAN_PY_EXE:-"python3"}
    export HOLOSCAN_PY_EXE
    HOLOSCAN_DOCKER_EXE=${HOLOSCAN_DOCKER_EXE:-"docker"}
    export HOLOSCAN_DOCKER_EXE

    DO_DRY_RUN="false"  # print commands but do not execute them. Used by run_command
}

################################################################################
# Utility functions
################################################################################


#######################################
# Check if current architecture is x86_64.
#
# Returns:
#   Exit code:
#     0 if $(uname -m) == "x86_64".
#     1 otherwise.
#######################################
checkif_x86_64() {
    if [ $(uname -m) == "x86_64" ]; then
        return 0
    else
        return 1
    fi
}

#######################################
# Check if current architecture is aarch64.
#
# Returns:
#   Exit code:
#     0 if $(uname -m) == "aarch64".
#     1 otherwise.
#######################################
checkif_aarch64() {
    if [ $(uname -m) == "aarch64" ]; then
        return 0
    else
        return 1
    fi
}

#######################################
# Get list of available commands from a given input file.
#
# Available commands and command summary are extracted by checking a pattern
# "_desc() { c_echo '".
# Section title is extracted by checking a pattern "# Section: ".
# This command is used for listing available commands in CLI.
#
# e.g.)
#   "# Section: String/IO functions"
#     => "# String/IO functions"
#   "to_lower_desc() { c_echo 'Convert to lower case"
#     => "to_lower ----------------- Convert to lower case"
#
# Arguments:
#   $1 - input file that defines commands
# Returns:
#   Print list of available commands from $1
#######################################
get_list_of_available_commands() {
    local mode="color"
    if [ "${1:-}" = "color" ]; then
        mode="color"
        shift
    elif [ "${1:-}" = "nocolor" ]; then
        mode="nocolor"
        shift
    fi

    local file_name="$1"
    if [ ! -e "$1" ]; then
        echo "$1 doesn't exist!"
    fi

    local line_str='--------------------------------'
    local IFS= cmd_lines="$(IFS= cat "$1" | grep -E -e "^(([[:alpha:]_[:digit:]]+)_desc\(\)|# Section: )" | sed "s/_desc() *{ *c_echo '/ : /")"
    local line
    while IFS= read -r line; do
        local cmd=$(echo "$line" | cut -d":" -f1)
        local desc=$(echo "$line" | cut -d":" -f2-)
        if [ "$cmd" = "# Section" ]; then
            c_echo ${mode} B "${desc}"
        else
            # there is no substring operation in 'sh' so use 'cut'
            local dash_line="$(echo "${line_str}" | cut -c ${#cmd}-)"  #  = "${line_str:${#cmd}}"
             c_echo ${mode} Y "   ${cmd}" w " ${dash_line} ${desc}"
        fi
        # use <<EOF, not '<<<"$cmd_lines"' to be executable in sh
    done <<EOF
$cmd_lines
EOF
}

my_cat_prefix() {
    local IFS
    local prefix="$1"
    local line
    while IFS= read -r line; do
        echo "${prefix}${line}" # -e option doesn't work in 'sh' so disallow escaped characters
    done <&0
}

c_str() {
    local old_color=39
    local old_attr=0
    local color=39
    local attr=0
    local text=""
    local mode="color"
    if [ "${1:-}" = "color" ]; then
        mode="color"
        shift
    elif [ "${1:-}" = "nocolor" ]; then
        mode="nocolor"
        shift
    fi

    for i in "$@"; do
        case "$i" in
            r|R)
                color=31
                ;;
            g|G)
                color=32
                ;;
            y|Y)
                color=33
                ;;
            b|B)
                color=34
                ;;
            p|P)
                color=35
                ;;
            c|C)
                color=36
                ;;
            w|W)
                color=37
                ;;

            z|Z)
                color=0
                ;;
        esac
        case "$i" in
            l|L|R|G|Y|B|P|C|W)
                attr=1
                ;;
            n|N|r|g|y|b|p|c|w)
                attr=0
                ;;
            z|Z)
                attr=0
                ;;
            *)
                text="${text}$i"
        esac
        if [ "${mode}" = "color" ]; then
            if [ ${old_color} -ne ${color} ] || [ ${old_attr} -ne ${attr} ]; then
                text="${text}\033[${attr};${color}m"
                old_color=$color
                old_attr=$attr
            fi
        fi
    done
    /bin/echo -en "$text"
}

c_echo() {
    # Select color/nocolor based on the first argument
    local mode="color"
    if [ "${1:-}" = "color" ]; then
        mode="color"
        shift
    elif [ "${1:-}" = "nocolor" ]; then
        mode="nocolor"
        shift
    else
        if [ ! -t 1 ]; then
            mode="nocolor"
        fi
    fi

    local old_opt="$(shopt -op xtrace)" # save old xtrace option
    set +x # unset xtrace

    if [ "${mode}" = "color" ]; then
        local text="$(c_str color "$@")"
        /bin/echo -e "$text\033[0m"
    else
        local text="$(c_str nocolor "$@")"
        /bin/echo -e "$text"
    fi
    eval "${old_opt}" # restore old xtrace option
}

echo_err() {
    >&2 echo "$@"
}

c_echo_err() {
    >&2 c_echo "$@"
}

printf_err() {
    >&2 printf "$@"
}

get_unused_ports() {
    local num_of_ports=${1:-1}
    local start=${2:-49152}
    local end=${3:-61000}
    comm -23 \
    <(seq ${start} ${end} | sort) \
    <(ss -tan | awk '{print $4}' | while read line; do echo ${line##*\:}; done | grep '[0-9]\{1,5\}' | sort -u) \
    | shuf | tail -n ${num_of_ports} # use tail instead head to avoid broken pipe in VSCode terminal
}

newline() {
    echo
}

info() {
    c_echo_err W "$(date -u '+%Y-%m-%d %H:%M:%S') [INFO] " Z "$@"
}

error() {
    c_echo_err R "$(date -u '+%Y-%m-%d %H:%M:%S') [ERROR] " Z "$@"
}

fatal() {
    if [ -n "$*" ]; then
        c_echo_err R "$(date -u '+%Y-%m-%d %H:%M:%S') [FATAL] " Z "$@"
        echo_err
    fi
    if [ -n "${SCRIPT_DIR}" ]; then
        exit 1
    else
        kill -INT $$  # kill the current process instead of exit in shell environment.
    fi
}

run_command() {
    local status=0
    local cmd="$*"

    if [ "${DO_DRY_RUN}" != "true" ]; then
        c_echo_err B "$(date -u '+%Y-%m-%d %H:%M:%S') " W "\$ " G "${cmd}"
    else
        c_echo_err B "$(date -u '+%Y-%m-%d %H:%M:%S') " C "[dryrun] " W "\$ " G "${cmd}"
    fi

    [ "$(echo -n "$@")" = "" ] && return 1 # return 1 if there is no command available

    if [ "${DO_DRY_RUN}" != "true" ]; then
        "$@"
        status=$?
    fi

    return $status
}

run_docker() {
    $(./run docker_cmd "-u $(id -u):$(id -g)") "$@"
}

#===============================================================================
# Section: Tool
#===============================================================================

install_gxf_desc() { c_echo 'Install GXF in the ${TOP}/.cache/gxf directory

This downloads and extracts the GXF package from NGC into "${TOP}/.cache/gxf".

Environments:
   GXF_TAG: the tag of the GXF package to download
   NGC_CLI_API_KEY: the API key of NGC (see https://ngc.nvidia.com/setup/api-key)
   NGC_CLI_ORG: the organization name of NGC CLI
   NGC_CLI_TEAM: the team name of NGC CLI
'
}
install_gxf() {
    local gxf_tag=${GXF_TAG:-2.5.0-6b2e34ec}
    local ngc_cli_org=${NGC_CLI_ORG:-nvidia}
    local ngc_cli_team=${NGC_CLI_TEAM:-clara-holoscan}
    local GXF_MODIFIED=false
    local ngc_orgteam_opt=""
    local download_command=ngc

    if [ -n "${NGC_CLI_API_KEY}" ]; then
        ngc_orgteam_opt="--org ${ngc_cli_org} --team ${ngc_cli_team}"
    fi

    mkdir -p ${TOP}/.cache
    pushd ${TOP}/.cache >/dev/null

    # Check if wget or ngc is installed
    if ! command -v ngc > /dev/null; then
      c_echo W "'ngc' cli command not found, using 'wget'"
      download_command=wget
      if ! command -v wget > /dev/null; then
          fatal R "Please install " W "wget" R " or the " W "ngc" R " command to run install_gxf. Follow the instructions in https://ngc.nvidia.com/setup/installers/cli to install NGC CLI."
      fi
    fi

    if [ ! -f gxf_x86_64_holoscan_sdk_v${gxf_tag}/gxf_*.tar.gz ]; then
        if [ "${download_command}" == "wget" ]; then
            run_command wget --content-disposition https://api.ngc.nvidia.com/v2/resources/${ngc_cli_org}/${ngc_cli_team}/gxf_x86_64_holoscan_sdk/versions/${gxf_tag}/zip -O gxf_x86_64_holoscan_sdk_${gxf_tag}.zip
            if [ $? -ne 0 ]; then
               fatal R "Unable to download " W "${ngc_cli_org}/${ngc_cli_team}/gxf_x86_64_holoscan_sdk:${gxf_tag}" R " Make sure you have access to NGC via your browser."
            fi
            run_command unzip gxf_x86_64_holoscan_sdk_${gxf_tag}.zip -d gxf_x86_64_holoscan_sdk_v${gxf_tag} \
             && run_command rm gxf_x86_64_holoscan_sdk_${gxf_tag}.zip
        else
            run_command ngc registry resource download-version ${ngc_orgteam_opt} "${ngc_cli_org}/${ngc_cli_team}/gxf_x86_64_holoscan_sdk:${gxf_tag}"
            if [ $? -ne 0 ]; then
                fatal R "Unable to download " W "${ngc_cli_org}/${ngc_cli_team}/gxf_x86_64_holoscan_sdk:${gxf_tag}" R ".
    If you are using a private registry, please try to configure NGC CLI ('ngc config set') or change NGC_CLI_API_KEY, NGC_CLI_ORG, and NGC_CLI_TEAM environment variables to the correct API key and organization name/team of NGC CLI." B '
        # configure NGC CLI
        ngc config set

        # Or, export NGC_CLI_API_KEY, NGC_CLI_ORG, and NGC_CLI_TEAM environment variables to the private registry.
        export NGC_CLI_API_KEY=<your-api-key> # see https://ngc.nvidia.com/setup/api-key
        export NGC_CLI_ORG=<ngc-organization>
        export NGC_CLI_TEAM=<ngc-team>'
            fi
        fi
        GXF_MODIFIED=true
    fi
    if [ ! -f gxf_arm64_holoscan_sdk_v${gxf_tag}/gxf_*.tar.gz ]; then
        if [ "${download_command}" == "wget" ]; then
            run_command wget --content-disposition https://api.ngc.nvidia.com/v2/resources/${ngc_cli_org}/${ngc_cli_team}/gxf_arm64_holoscan_sdk/versions/${gxf_tag}/zip -O gxf_arm64_holoscan_sdk_${gxf_tag}.zip
            if [ $? -ne 0 ]; then
               fatal R "Unable to download " W "${ngc_cli_org}/${ngc_cli_team}/gxf_arm64_holoscan_sdk:${gxf_tag}" R " Make sure you have access to NGC via your browser."
            fi
            run_command unzip gxf_arm64_holoscan_sdk_${gxf_tag}.zip -d gxf_arm64_holoscan_sdk_v${gxf_tag} \
             && run_command rm gxf_arm64_holoscan_sdk_${gxf_tag}.zip
        else
            run_command ngc registry resource download-version ${ngc_orgteam_opt} "${ngc_cli_org}/${ngc_cli_team}/gxf_arm64_holoscan_sdk:${gxf_tag}"
            if [ $? -ne 0 ]; then
                fatal R "Unable to download " W "${ngc_cli_org}/${ngc_cli_team}/gxf_arm64_holoscan_sdk:${gxf_tag}" R ".
    If you are using a private registry, please try to configure NGC CLI ('ngc config set') or change NGC_CLI_API_KEY, NGC_CLI_ORG, and NGC_CLI_TEAM environment variables to the correct API key and organization name/team of NGC CLI." B '
        # configure NGC CLI
        ngc config set

        # Or, export NGC_CLI_API_KEY, NGC_CLI_ORG, and NGC_CLI_TEAM environment variables to the private registry.
        export NGC_CLI_API_KEY=<your-api-key> # see https://ngc.nvidia.com/setup/api-key
        export NGC_CLI_ORG=<ngc-organization>
        export NGC_CLI_TEAM=<ngc-team>'
            fi
        fi
        GXF_MODIFIED=true
    fi

    if [ "${GXF_MODIFIED}" == "true" ] || [ ! -e ${TOP}/.cache/gxf/gxf ]; then
        run_command mkdir -p ${TOP}/.cache/gxf
        run_command rm -rf ${TOP}/.cache/gxf/*

        run_command tar -zxf gxf_x86_64_holoscan_sdk_v${gxf_tag}/gxf_*.tar.gz -C ${TOP}/.cache/gxf --strip-components=1
        run_command tar -zxf gxf_arm64_holoscan_sdk_v${gxf_tag}/gxf_*.tar.gz -C ${TOP}/.cache/gxf --strip-components=1

        # Patch GXF to remove its support for complex cuda primitives,
        # due to limitation in the CUDA Toolkit 11.6 with C++17
        # Support will be added when upgrading to CUDA Toolkit 11.7+
        # with libcu++ 1.8.0+: https://github.com/NVIDIA/libcudacxx/pull/234
        run_command patch -suNb ${TOP}/.cache/gxf/gxf/std/tensor.hpp ${TOP}/patches/gxf_remove_complex_primitives_support.patch
        run_command mv ${TOP}/.cache/gxf/gxf/std/complex.hpp ${TOP}/.cache/gxf/gxf/std/complex.hpp.orig
    else
        c_echo_err W "GXF is already installed"
    fi

    popd >/dev/null

}

#===============================================================================
# Section: Build
#===============================================================================

get_buildtype_str() {
    local build_type="${1:-}"
    local build_type_str

    case "${build_type}" in
        debug|Debug)
            build_type_str="Debug"
            ;;
        release|Release)
            build_type_str="Release"
            ;;
        rel-debug|RelWithDebInfo)
            build_type_str="RelWithDebInfo"
            ;;
        *)
            build_type_str="${CMAKE_BUILD_TYPE:-Release}"
            ;;
    esac

    echo -n "${build_type_str}"
}

get_platform_str() {
    local platform="${1:-}"
    local platform_str

    case "${platform}" in
        amd64|x86_64|x86|linux/amd64)
            platform_str="linux/amd64"
            ;;
        arm64|aarch64|arm|linux/arm64)
            platform_str="linux/arm64"
            ;;
    esac

    echo -n "${platform_str}"
}

get_gpu_str() {
    local gpu="${1:-}"
    local gpu_str=$gpu

    case "${gpu}" in
        igpu|iGPU|integrated|Jetson)
            gpu_str="igpu"
            ;;
        dgpu|dGPU|discrete|RTX)
            gpu_str="dgpu"
            ;;
    esac

    echo -n "${gpu_str}"
}

get_host_gpu() {
    if lsmod | grep -q nvidia_drm && command -v nvidia-smi > /dev/null; then
        echo -n "dgpu"
    elif lsmod | grep -q nvgpu ; then
        echo -n "igpu"
    else
        c_echo_err Y "Could not find any GPU drivers on host. Defaulting build to target dGPU/CPU stack."
        echo -n "dgpu"
    fi
}

get_cuda_archs() {
    local cuda_archs="${1:-}"

    case "${cuda_archs}" in
        native|NATIVE)
            cuda_archs_str="NATIVE"
            ;;
        all|ALL)
            cuda_archs_str="ALL"
            ;;
        *)
            cuda_archs_str="${1:-}"
            ;;
    esac

    echo -n "${cuda_archs_str}"
}

clear_cache_desc() { c_echo 'Clear cache folders (including build/install folders)
'
}
clear_cache() {
    c_echo W "Clearing cache..."
    run_command rm -rf ${TOP}/build
    run_command rm -rf ${TOP}/install
    run_command rm -rf ${TOP}/build-*
    run_command rm -rf ${TOP}/install-*

    run_command rm -rf ${TOP}/.cache/ccache
    run_command rm -rf ${TOP}/.cache/cpm
    run_command rm -rf ${TOP}/.cache/gxf
}

setup_desc() { c_echo 'Setup development environment
'
}
setup() {
    c_echo W "Setup development environment..."

    if ! command -v ${HOLOSCAN_DOCKER_EXE} > /dev/null; then
        fatal G "${HOLOSCAN_DOCKER_EXE}" W " doesn't exists. Please install NVIDIA Docker!"
    fi

    if ! groups | grep -q docker; then
        c_echo_err G "groups" W " doesn't contain 'docker' group. Please add 'docker' group to your user."
        fatal G "groups" W " doesn't contain 'docker' group. Please add 'docker' group to your user." B '
    # Create the docker group.
    sudo groupadd docker
    # Add your user to the docker group.
    sudo usermod -aG docker $USER
    newgrp docker
    docker run hello-world'
    fi

    if checkif_x86_64 && [ -n "${HOLOSCAN_BUILD_PLATFORM}" ] && [ ! -f /proc/sys/fs/binfmt_misc/qemu-aarch64 ]; then
        fatal G "qemu-aarch64" W " doesn't exists. Please install qemu with binfmt-support to run Docker container with aarch64 platform" B '
    # Install the qemu packages
    sudo apt-get install qemu binfmt-support qemu-user-static
    # Execute the registering scripts
    docker run --rm --privileged multiarch/qemu-user-static --reset -p yes'
    fi
}

build_image_desc() { c_echo 'Build dev image
  --platform [linux/amd64 | linux/arm64] : Specify the platform (for cross-compilation)
        Default: current host platform
        Associated environment variable: HOLOSCAN_BUILD_PLATFORM
  --gpu [igpu | dgpu] : Specify the GPU stack (integrated vs discrete)
        Default: current host platform
        Associated environment variable: HOLOSCAN_BUILD_GPU_TYPE
'
}
build_image() {
    # Parse env variables first or set default values
    local platform="${HOLOSCAN_BUILD_PLATFORM:-$(get_platform_str $(uname -p))}"
    local gpu=$(get_gpu_str ${HOLOSCAN_BUILD_GPU_TYPE:-$(get_host_gpu)})

    # Parse CLI arguments next
    ARGS=("$@")
    local i
    local arg
    for i in "${!ARGS[@]}"; do
        arg="${ARGS[i]}"
        if [ "$arg" = "--platform" ]; then
           platform=$(get_platform_str "${ARGS[i+1]}")
        fi
        if [ "$arg" = "--gpu" ]; then
           gpu=$(get_gpu_str "${ARGS[i+1]}")
        fi
    done

    # Docker build
    run_command export DOCKER_BUILDKIT=1
    run_command docker build \
        --build-arg BUILDKIT_INLINE_CACHE=1 \
        --build-arg GPU_TYPE=${gpu} \
        --platform ${platform} \
        --network=host \
        -t holoscan-sdk-dev \
        ${TOP}
}

build_desc() { c_echo 'Build the project

This command will build the project with optional configuration arguments.

Arguments:
  --platform [linux/amd64 | linux/arm64] : Specify the platform (for cross-compilation)
        Default: current host platform
        Associated environment variable: HOLOSCAN_BUILD_PLATFORM
  --gpu [igpu | dgpu] : Specify the GPU stack (integrated vs discrete)
        Default: current host GPU
        Associated environment variable: HOLOSCAN_BUILD_GPU_TYPE
  --cudaarchs [native | all | <custom_arch_list>]
        Default: native
        Associated environment variable: CMAKE_CUDA_ARCHITECTURES
  --type [debug | release | rel-debug] : Specify the type of build
        Default: release
        Associated environment variable: CMAKE_BUILD_TYPE
  --buildpath <build_directory> : Change the build path.
        Default: build
        Associated environment variable: CMAKE_BUILD_PATH
  --installprefix <install_directory> : Specify the install directory
        Default: install
        Associated environment variable: CMAKE_INSTALL_PREFIX
  --reconfigure: Force reconfiguration of the CMake project. By default CMake is only run if
        a CMakeCache.txt does not exist in the build directory or if CMake detects a reconfigure
        is needed.
        Default: false
'
}
build() {
    # Parse env variables first or set default values
    local platform="${HOLOSCAN_BUILD_PLATFORM:-$(get_platform_str $(uname -p))}"
    local gpu=$(get_gpu_str ${HOLOSCAN_BUILD_GPU_TYPE:-$(get_host_gpu)})
    local cuda_archs="${CMAKE_CUDA_ARCHITECTURES:-NATIVE}"
    local build_type="${CMAKE_BUILD_TYPE:-release}"
    local build_path="${CMAKE_BUILD_PATH:-build}"
    local install_prefix="${CMAKE_INSTALL_PREFIX:-install}"
    local reconfigure=false

    # Parse CLI arguments next
    ARGS=("$@")
    local i
    local arg
    for i in "${!ARGS[@]}"; do
        arg="${ARGS[i]}"
        if [ "$arg" = "--platform" ]; then
           platform=$(get_platform_str "${ARGS[i+1]}")
        fi
        if [ "$arg" = "--gpu" ]; then
           gpu=$(get_gpu_str "${ARGS[i+1]}")
        fi
        if [ "$arg" = "--cudaarchs" ]; then
           cuda_archs=$(get_cuda_archs "${ARGS[i+1]}")
           reconfigure=true
        fi
        if [ "$arg" = "--type" ]; then
           build_type=$(get_buildtype_str "${ARGS[i+1]}")
           reconfigure=true
        fi
        if [ "$arg" = "--buildpath" ]; then
           build_path="${ARGS[i+1]}"
        fi
        if [ "$arg" = "--installprefix" ]; then
           install_prefix="${ARGS[i+1]}"
        fi
        if [ "$arg" = "--reconfigure" ]; then
           reconfigure=true
        fi
    done

    setup
    build_image --platform ${platform} --gpu ${gpu}
    install_gxf

    # DOCKER PARAMETERS
    #
    # -it
    #   Making the container interactive allows cancelling the build
    #
    # --rm
    #   Deletes the container after the command runs
    #
    # -u $(id -u):$(id -g)
    #   Ensures the generated files (build, install...) are owned by $USER and not root
    #
    # -v ${TOP}:/workspace/holoscan-sdk
    #   Mount the source directory
    #
    # -w /workspace/holoscan-sdk
    #   Start in the source directory
    #
    # -e CMAKE_BUILD_PARALLEL_LEVEL=$(nproc)
    #   Optimize the number of concurrent processes when building
    #
    #
    # CMAKE PARAMETERS
    #
    # -S . -B ${build_path}
    #   Generic configuration
    #
    # -D CMAKE_BUILD_TYPE=${build_type}
    #   Define the build type (release, debug...). Can be passed as env to docker also.
    #
    # -D CMAKE_CUDA_ARCHITECTURES=${cuda_archs}
    #   Define the cuda architectures to build for (NATIVE, ALL, custom). If custom or ALL, the
    #   last arch will be used for real and virtual architectures (PTX, forward compatible) while
    #   the previous archs will be real only.
    #
    run_command ${HOLOSCAN_DOCKER_EXE} run --rm --net host -it \
        -u $(id -u):$(id -g) \
        -v ${TOP}:/workspace/holoscan-sdk \
        -w /workspace/holoscan-sdk \
        -e CMAKE_BUILD_PARALLEL_LEVEL=$(nproc) \
        holoscan-sdk-dev \
        bash -c "
            if [ ! -f '${build_path}/build.ninja' ] || ${reconfigure} ; then \
                cmake -S . -B ${build_path} -G Ninja \
                  -D CMAKE_CUDA_ARCHITECTURES=\"${cuda_archs}\" \
                  -D CMAKE_BUILD_TYPE=${build_type}; \
            fi \
            && cmake --build ${build_path} -j \
            && cmake --install ${build_path} --prefix ${install_prefix} --component holoscan-core \
            && cmake --install ${build_path} --prefix ${install_prefix} --component holoscan-gxf_extensions \
            && cmake --install ${build_path} --prefix ${install_prefix} --component holoscan-examples \
            && cmake --install ${build_path} --prefix ${install_prefix} --component holoscan-gxf_libs \
            && cmake --install ${build_path} --prefix ${install_prefix} --component holoscan-gxf_bins \
            && cmake --install ${build_path} --prefix ${install_prefix} --component holoscan-modules \
            && cmake --install ${build_path} --prefix ${install_prefix} --component holoscan-dependencies \
            && cmake --install ${build_path} --prefix ${install_prefix} --component holoscan-python_libs
        "
}

#===============================================================================
# Section: Test
#===============================================================================

lint_desc() { c_echo 'Lint the repository

Python linting: black, isort, ruff
C++ linting: cpplint
Spelling: codespell

Arguments:
  $@ - directories to lint (default: .)
'
}

lint() {
    local DIR_TO_RUN=${@:-"."}

    # We use $(command) || exit_code=1 to run all linting tools, and exit
    # with failure after all commands were executed if any of them failed
    local exit_code=0

    pushd ${TOP} > /dev/null

    c_echo W "Linting Python"
    run_command ruff $DIR_TO_RUN || exit_code=1
    run_command ${HOLOSCAN_PY_EXE} -m isort -c $DIR_TO_RUN || exit_code=1
    run_command ${HOLOSCAN_PY_EXE} -m black --check $DIR_TO_RUN || exit_code=1

    c_echo W "Linting C++"
    # We use `grep -v` to hide verbose output that drowns actual errors
    # Since we care about the success/failure of cpplitn and not of grep, we:
    #  1. use `set -o pipefail` to fail if `cpplint` fails
    #  2. use `grep -v ... || true` to ignore whether grep hid any output
    run_command set -o pipefail; ${HOLOSCAN_PY_EXE} -m cpplint \
            --exclude .cache \
            --exclude build \
            --exclude install \
            --exclude build-\* \
            --exclude install-\* \
            --recursive $DIR_TO_RUN \
        | { grep -v "Ignoring\|Done processing" || true; } || exit_code=1


    c_echo W "Code spelling"
    run_command codespell $DIR_TO_RUN || exit_code=1

    popd > /dev/null

    exit $exit_code
}

coverage_desc() { c_echo 'Execute code coverage
  --type [debug | release | rel-debug] : Specify the type of build
        Default: release
        Associated environment variable: CMAKE_BUILD_TYPE
  --buildpath <build_directory> : Change the build path.
        Default: build
        Associated environment variable: CMAKE_BUILD_PATH
'
}
coverage() {
    # Parse env variables first or set default values
    local build_type="${CMAKE_BUILD_TYPE:-release}"
    local build_path="${CMAKE_BUILD_PATH:-build}"

    # Parse CLI arguments next
    ARGS=("$@")
    local i
    local arg
    for i in "${!ARGS[@]}"; do
        arg="${ARGS[i]}"
        if [ "$arg" = "--type" ]; then
           build_type=$(get_buildtype_str "${ARGS[i+1]}")
        fi
        if [ "$arg" = "--buildpath" ]; then
           build_path="${ARGS[i+1]}"
        fi
    done
    local working_dir=${1:-${build_path}}

    # Find the nvidia_icd.json file which could reside at different paths
    # Needed due to https://github.com/NVIDIA/nvidia-container-toolkit/issues/16
    nvidia_icd_json=$(find /usr/share /etc -path '*/vulkan/icd.d/nvidia_icd.json' -type f,l -print -quit 2>/dev/null | grep .) || (echo "nvidia_icd.json not found" >&2 && false)

    run_command ${HOLOSCAN_DOCKER_EXE} run --rm --net host \
        -u $(id -u):$(id -g) \
        -v ${TOP}:/workspace/holoscan-sdk \
        -w /workspace/holoscan-sdk \
        --runtime=nvidia \
        -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display \
        -v /tmp/.X11-unix:/tmp/.X11-unix \
        -e DISPLAY=$DISPLAY \
        -v $nvidia_icd_json:$nvidia_icd_json:ro \
        -e HOLOSCAN_LIB_PATH=/workspace/holoscan-sdk/${working_dir}/lib \
        -e HOLOSCAN_SAMPLE_DATA_PATH=/workspace/holoscan-sdk/data \
        -e HOLOSCAN_TESTS_DATA_PATH=/workspace/holoscan-sdk/tests/data \
        -e HOLOSCAN_LOG_LEVEL=INFO \
        -e PYTHONPATH=/workspace/holoscan-sdk/${working_dir}/python/lib \
        -e CMAKE_BUILD_PARALLEL_LEVEL=$(nproc) \
        holoscan-sdk-dev \
        bash -c "
            cmake -S . -B ${build_path} -G Ninja \
                -D HOLOSCAN_BUILD_COVERAGE=ON \
                -D CMAKE_BUILD_TYPE=${build_type} \
            && cmake --build ${build_path} \
            && cmake --build ${build_path} --target coverage
        "
}

# test_desc() { c_echo 'Execute test cases

# Arguments:
#   $1 - subcommand [all] (default: all)
#   $2 - test_type [all|unit|integration|system|performance] (default: all)
#   $3 - test_component [all] (default: all)
# '
# }
# test() {
#     local subcommand="${1:-all}"
#     local test_type="${2:-all}"
#     shift;

# }

#===============================================================================
# Section: Launch
#===============================================================================

launch_desc() { c_echo 'Launch Docker container

Export CMAKE_BUILD_PATH (default: "build") to change the build path.
  e.g.,

    export CMAKE_BUILD_PATH=build-arm64

Arguments:
    $1 - Working directory (e.g, "install" => "/workspace/holoscan-sdk/install")
         Default: "build"
'
}
launch() {
    local build_path="${CMAKE_BUILD_PATH:-build}"
    local working_dir=${1:-${build_path}}
    local mount_device_opt=""

    # Skip the first argument to pass the remaining arguments to the docker command.
    if [ -n "$1" ]; then
        shift
    fi

    setup

    # Allow connecting from docker. This is not needed for WSL2 (`SI:localuser:wslg` is added by default)
    run_command xhost +local:docker

    for i in 0 1 2 3; do
        if [ -e /dev/video${i} ]; then
            mount_device_opt+=" --device /dev/video${i}:/dev/video${i}"
        fi
        if [ -e /dev/ajantv2${i} ]; then
            mount_device_opt+=" --device /dev/ajantv2${i}:/dev/ajantv2${i}"
        fi
    done

    c_echo W "Launching (mount_device_opt:" G "${mount_device_opt}" W ")..."

    # Find the nvidia_icd.json file which could reside at different paths
    # Needed due to https://github.com/NVIDIA/nvidia-container-toolkit/issues/16
    nvidia_icd_json=$(find /usr/share /etc -path '*/vulkan/icd.d/nvidia_icd.json' -type f,l -print -quit 2>/dev/null | grep .) || (echo "nvidia_icd.json not found" >&2 && false)

    # DOCKER PARAMETERS
    #
    # -it
    #   The container needs to be interactive to be able to interact with the X11 windows
    #
    # --rm
    #   Deletes the container after the command runs
    #
    # -u $(id -u):$(id -g)
    # -v /etc/group:/etc/group:ro
    # -v /etc/passwd:/etc/passwd:ro
    #   Ensures the generated files (build, install...) are owned by $USER and not root,
    #   and provide the configuration files to avoid warning for user and group names
    #
    # -v ${TOP}:/workspace/holoscan-sdk
    #   Mount the source directory
    #
    # -w /workspace/holoscan-sdk/${working_dir}
    #   Start in the build or install directory
    #
    # --runtime=nvidia \
    # -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display
    #   Enable GPU acceleration
    #
    # -v /tmp/.X11-unix:/tmp/.X11-unix
    # -e DISPLAY
    #   Enable graphical applications
    #
    # -v $nvidia_icd_json:$nvidia_icd_json:ro
    #   Bind NVIDIA's Vulkan installable client driver to run Vulkan
    #   Needed due to https://github.com/NVIDIA/nvidia-container-toolkit/issues/16
    #   The configurations files are installed to different locations when installing
    #   with deb packages or with run files, so we look at both places
    #
    # --device /dev/video${i}:/dev/video${i}
    #   Bind video capture devices for V4L2
    #
    # --device /dev/ajantv2${i}:/dev/ajantv2${i}
    #   Bind AJA capture cards for NTV2
    #
    # -e PYTHONPATH
    # -e HOLOSCAN_LIB_PATH
    # -e HOLOSCAN_SAMPLE_DATA_PATH
    #   Define paths needed by the python applications
    #
    # -e CUPY_CACHE_DIR
    #   Define path for cupy' kernel cache, needed since $HOME does
    #   not exist when running with `-u id:group`

    run_command ${HOLOSCAN_DOCKER_EXE} run -it --rm --net host \
        -u $(id -u):$(id -g) \
        -v /etc/group:/etc/group:ro \
        -v /etc/passwd:/etc/passwd:ro \
        -v ${TOP}:/workspace/holoscan-sdk \
        -w /workspace/holoscan-sdk/${working_dir} \
        --runtime=nvidia \
        -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display \
        -v /tmp/.X11-unix:/tmp/.X11-unix \
        -e DISPLAY \
        ${mount_device_opt} \
        -v $nvidia_icd_json:$nvidia_icd_json:ro \
        -e PYTHONPATH=/workspace/holoscan-sdk/${working_dir}/python/lib \
        -e HOLOSCAN_LIB_PATH=/workspace/holoscan-sdk/${working_dir}/lib \
        -e HOLOSCAN_SAMPLE_DATA_PATH=/workspace/holoscan-sdk/data \
        -e HOLOSCAN_TESTS_DATA_PATH=/workspace/holoscan-sdk/tests/data \
        -e CUPY_CACHE_DIR=/workspace/holoscan-sdk/.cupy/kernel_cache \
        holoscan-sdk-dev "$@"
}

vscode_desc() { c_echo 'Launch VSCode in DevContainer

Launch a VSCode instance in a Docker container with the development environment.
'
}
vscode() {
    if ! command -v code > /dev/null; then
        fatal R "Please install " W "VSCode" R " to use VSCode DevContainer. Follow the instructions in https://code.visualstudio.com/Download"
    fi

    local workspace_path=$(git rev-parse --show-toplevel 2> /dev/null || dirname $(realpath -s $0))
    local workspace_path_hex

    if [ "${workspace_path}" != "${TOP}" ]; then
        # Set the environment variable 'HOLOSCAN_PUBLIC_FOLDER' to the resolved path relative to the
        # workspace path if the workspace path is different than the TOP folder so that
        # the DevContainer can build files under the Holoscan source root folder while mounting the
        # workspace folder.
        run_command export HOLOSCAN_PUBLIC_FOLDER=$(realpath --relative-to=${workspace_path} ${TOP})
    fi

    # Allow connecting from docker. This is not needed for WSL2 (`SI:localuser:wslg` is added by default)
    run_command xhost +local:docker

    # Install the VSCode Remote Development extension.
    # Checking if the extension is already installed is slow, so we always install it so that
    # the installation can be skipped if the extension is already installed.
    run_command code --force --install-extension ms-vscode-remote.vscode-remote-extensionpack

    # For now, there is no easy way to launch DevContainer directly from the command line.
    # ('devcontainer' CLI is only available after manually executing 'Remote-Containers: Install devcontainer CLI' command)
    # Here, we launch VSCode with .devcontainer/devcontainer.json with '--folder-uri' option to open the current folder in the container.
    # See https://github.com/microsoft/vscode-remote-release/issues/2133
    workspace_path_hex=$(echo -n ${workspace_path} | xxd -p)
    workspace_path_hex="${workspace_path_hex//[[:space:]]/}"
    c_echo B "Local workspace path: " W "${workspace_path} (${workspace_path_hex})"
    run_command code --folder-uri "vscode-remote://dev-container+${workspace_path_hex}/workspace/holoscan-sdk"
}

vscode_remote_desc() { c_echo 'Attach to the existing VSCode DevContainer

This command is useful when you want to attach to the existing VSCode DevContainer from the remote machine.

1) Launch VSCode DevContainer on the local machine with "./run vscode" command.
2) In the remote machine, launch VSCode Remote-SSH extension to connect to the local machine.
3) Launch this command on the remote machine to attach to the existing VSCode DevContainer in the local machine.
'
}
vscode_remote() {
    if ! command -v code > /dev/null; then
        fatal R "Please install " W "VSCode" R " to use VSCode DevContainer. Follow the instructions in https://code.visualstudio.com/Download"
    fi

    local workspace_path=$(git rev-parse --show-toplevel 2> /dev/null || dirname $(realpath -s $0))

    local container_name=$(run_command docker ps --filter "label=devcontainer.local_folder=${workspace_path}" --format "{{.Names}}" | head -1)
    if [ -z "${container_name}" ]; then
        fatal R "No DevContainer (with the label 'devcontainer.local_folder=${workspace_path}') is found in the local machine. Please launch VSCode DevContainer with " W "./run vscode" R " command first."
    fi
    c_echo W "Container name: " y "${container_name}"

    # The following command is based on the information of the following issue:
    #   https://github.com/microsoft/vscode-remote-release/issues/2133#issuecomment-618328138
    python - > /tmp/holoscan_remote_uri.txt << EOF
import json
from urllib.parse import urlunparse
from subprocess import check_output
hostname = check_output("hostname" , shell=True).decode('utf-8').strip()
remote_description = json.dumps({"containerName": f"/${container_name}", "settings":{"host": f"ssh://{hostname}"}})
remote_uri = urlunparse(('vscode-remote', f'attached-container+{remote_description.encode("utf8").hex()}', '/workspace/holoscan-sdk', '', '', ''))
print(remote_uri)
EOF
    local remote_uri=$(cat /tmp/holoscan_remote_uri.txt)
    c_echo W "Remote URI: " y "${remote_uri}"

    # VSCode's attached-container mode does not support forwarding SSH keys to the container well.
    # For the reason, we need to copy SSH keys to the container so that the container can access
    # the local machine's SSH keys.
    c_echo W "Copying SSH keys to the container..."
    for i in identity id_rsa id_ed25519 id_ed25519_sk id_dsa id_cdsa id_ecdsa_sk; do
        if [ -e ~/.ssh/${i} ]; then
            run_command docker cp ~/.ssh/${i} ${container_name}:/home/holoscan-sdk/.ssh/
        fi
    done;

    run_command code --folder-uri ${remote_uri}
}

#===============================================================================

parse_args() {
    local OPTIND
    while getopts 'yh' option;
    do
        case "${option}" in
            y)
                ALWAYS_YES="true"
                ;;
            h)
                print_usage
                if [ -n "${SCRIPT_DIR}" ]; then
                    exit 1
                fi
                ;;
            *)
                ;;
        esac
    done
    shift $((OPTIND-1))

    CMD="$1"
    shift

    ARGS=("$@")
    # Check if the command has `--help`, `-h`, or `--dryrun`, and override the CMD
    local i
    local arg
    local unset_pos
    for i in "${!ARGS[@]}"; do
        arg="${ARGS[i]}"
        if [ "$arg" = "--help" ] || [ "$arg" = "-h" ]; then
            ARGS=("$CMD")
            CMD="help"
            break
        fi
        if [ "$arg" = "--dryrun" ]; then
            unset_pos=$i
            DO_DRY_RUN="true"  # set to true to print commands to screen without running
        fi
    done
    if [ "${unset_pos}" ]; then
        unset 'ARGS[unset_pos]'
    fi
}

print_usage() {
    set +x
    echo_err
    echo_err "USAGE: $0 [command] [arguments]..."
    echo_err ""
    c_echo_err W "Global Arguments"
    c_echo_err "  --help, -h      : Print help messages for [command]"
    c_echo_err "  --dryrun        : Print commands to screen without running"
    echo_err
    c_echo_err W "Command List"
    c_echo_err Y "    help  " w "----------------------------  Print detailed description for a given argument (command name)"
    echo_err "$(get_list_of_available_commands color "${RUN_SCRIPT_FILE}" | my_cat_prefix " ")"
    echo_err
}

print_cmd_help_messages() {
    local cmd="$1"
    if [ -n "${cmd}" ]; then
        if type ${cmd}_desc > /dev/null 2>&1; then
            ${cmd}_desc
            exit 0
        else
            c_echo_err R "Command '${cmd}' doesn't exist!"
            exit 1
        fi
    fi
    print_usage
    return 0
}

main() {
    local ret=0
    parse_args "$@"

    case "$CMD" in
        help)
            print_cmd_help_messages "${ARGS[@]}"
            exit 0
            ;;
        ''|main)
            print_usage
            ;;
        *)
            if type ${CMD} > /dev/null 2>&1; then
                "$CMD" "${ARGS[@]}"
            else
                print_usage
                exit 1
            fi
            ;;
    esac
    ret=$?
    if [ -n "${SCRIPT_DIR}" ]; then
        exit $ret
    fi
}

init_globals

if [ -n "${SCRIPT_DIR}" ]; then
    main "$@"
fi

#===============================================================================
# Description template
#===============================================================================
# Globals:
#   HOLOSCAN_OS
#   HOLOSCAN_TARGET
#   HOLOSCAN_USER (used if HOLOSCAN_OS is "linux")
#   HOLOSCAN_HOST (used if HOLOSCAN_OS is "linux")
# Arguments:
#   Command line to execute
# Returns:
#   Outputs print messages during the execution (stdout->stdout, stderr->stderr).
#
#   Note:
#     This command removes "\r" characters from stdout.
#
#   Exit code:
#     exit code returned from executing a given command
